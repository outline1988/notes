## 强化学习真的能用在抗干扰问题上吗？

### 观点1：通过找到环境的规律来实现抗干扰

通过调整捷变频信号的频率编码方式来主动抗干扰本质上是一个波形优化的问题，我认为波形优化问题的整体框架如下

![image-20240321224552076](C:\Users\outline\AppData\Roaming\Typora\typora-user-images\image-20240321224552076.png)

传统的波形优化问题将环境视为一个复杂的**已知函数（或者系统）**，其输入单个脉冲的发射波形，返回一个接收波形，也即从发射端到接收端再到最后输出结果这一正向传播的过程是清晰且明确的，由此便可以通过各种已知的优化方法以某一优化准则来完成对于发射端的波形和接收端信号处理方法的优化（当然这里主要对发射波形进行优化，故假设接收端对于每一种波形都有固定合适的信号处理方法）。

在抗干扰问题的假设下，环境这个极为复杂的函数不再为已知，如此，便希望使用人工智能的方法来克服环境未知的困难。那么该如何使用人工智能的方法解决这一问题呢？

当前将强化学习运用到抗干扰的基本思路为，将雷达与干扰机交互的过程建模为MDP

- 状态空间：过去多个（通常为3个）脉冲的频点编码情况和每个历史脉冲对应的干扰频点情况；
- 动作空间：将要发射的脉冲中子脉冲频率编码的情况；
- 奖励：时频域未遮盖率（目标就是让时域和频域至少有一个可分域，也即让发射子脉冲的频点和干扰的频点不在同一段时间内相同）。
- 转移函数：这里是我觉得最为争议的地方。

先入为主的观点来看，我们是希望雷达在与环境交互的过程中（也即不断地试错的过程中），找到环境（干扰机）这个未知切复杂函数的规律或者说特征，也就是说，最好的情况就是给定环境一个输入，我们能猜测出这个环境的输出，从而找到与之对抗的策略从而实现抗干扰。

*但是将抗干扰问题这样建模真的是按照这样子的逻辑运作吗？*
状态空间压缩的观点可以将相似规律的函数压缩为一类。

*如果按照这个思路，是否可以先使用有监督学习的方法将环境这个环境的函数拟合出来，然后再由后台搜索出合适的频点呢？*

### 观点2：从强化学习是如何最大化奖励来讨论

强化学习最显著的两个特征：一是试错，二是延迟奖励。前者告诉我们智能体获得样本的方式，后者揭示了强化学习序贯决策的概念。

马尔可夫决策过程是强化学习的基本数学框架，我认为其核心在于我们能够通过改变策略（某个状态选择怎样的动作）来控制状态转移，简单来说就是我们拥有一定的自由度能使得一个马尔可夫过程具有不同的状态转移特性（比如在grid word中，每一个小方格就是一个状态，我们能够通过选择上下左右的动作来控制智能体从某个格子转移到另一个格子）。

在value based的强化学习算法中，智能体与环境进行交互的数据都用来估计每个状态的价值函数（可以理解为当前状态的好坏程度，值越大，说明智能体到达这个状态时，更有实现最大化奖励的可能），此时好的策略能够使得智能体从当前状态转移到能到达的最好状态。尽管用到了深度学习技术，也仅是将神经网络用于价值函数的近似中，也即使用函数近似的方法认为一些状态是相似的，而神经网络的作用就是泛化出哪些状态是相似的，哪些不是，从而实现状态空间的压缩。

所以综上所述，value based的算法隐含假设是每个状态都具有不同的价值，而策略就是为了能够让智能体转移到更好的状态，从而实现奖励最大化。

~~*但是反观抗干扰的MDP建模，多个脉冲的频点编码和对应干扰作为状态，能够体现出哪个状态是好的哪个状态是坏的吗？*~~
虽然抗干扰问题下的状态无法像grid word中的状态这样有明显的好坏，但是某个状态动作对是有好坏的。

~~*既然如此，为何实际这样做了又能够出结果呢？*~~
因为有深度学习学习的泛化能力能够使得状态空间和动作空间降维

### 观点3：有监督学习的特征提取能力将状态空间和动作空间降维

建模剪刀石头布为MDP过程，手动生成轨迹而受到的启发。

前面提到，value based的强化学习算法在状态空间过大的时候，会使用函数近似的方法拟合价值函数$v_{\pi}(s)$。这样操作的隐含假设是认为状态空间虽然大，但是状态与状态之间是具有相似性的，通过近似函数提取特征的泛化能力来降维状态空间。例如，在抗干扰的问题中，如果干扰机的干扰策略仅仅只是子脉冲级别的，那么状态选取为历史三个脉冲和对应干扰频点实际上对于决策没有多大帮助，所以训练到最后，状态空间一定会压缩至1（不好说，也许只是子状态空间被压缩为1），最终转换为了一个k-armed bandit问题。又例如估计$q_{\pi}(s, a)$时，如果动作空间很大，并且有一些的动作选择会造成类似结果，这是这些类似的动作就会被视为相似的，从而达到动作空间降维的目的。

简单来说，强化学习的过程就是：**这个状态下我试过这个动作，这个动作不好，下次遇到类似的状态时，我就不选择这个动作了。**那么在复杂干扰的情况下agent是否还能学习到抗干扰策略？我觉得定性的判断就是在不同的干扰策略下，是否会对于agent做出决策所依赖的状态产生某种特征层面上的不同，即是否不同的干扰就会使得状态有不同的特征表示。若不同干扰能够对状态空间具有特征层面上的不同，则神经网络能够压缩出的不同状态，从而以此进行不同的抗干扰决策。总的来说，这样的状态空间设置泛化程度还挺大的。

上一段说到，希望不同的干扰能够产生不同特征的状态，故实现了一个大的状态空间上的压缩，在压缩后的每一个特征状态中，其都相当于是一个k-armed bandit问题。

如果真的不同的干扰会产生不同特征的状态，那么元学习的意义其实挺大的，因为元学习通过预先训练提前进行了某种程度上的状态空间降维，由此能够在实际的问题中更加快速的适应新的干扰策略。

*如果强化学习抗干扰的过程正如上面所说，那么本质上就是一个特征提取加试错的过程，在干扰策略不变的情况下，雷达学习到的策略是如何将状态空间和数据空间降维是比较直观的，那么在干扰策略按照一定规律变化，这样的方法又有办法能够实现策略的学习吗？*

*将状态空间选择为历史多个子脉冲的频点往往是非常冗余的，这就会导致需要更多的数据进行泛化，也就需要更多的时间（在该问题中，数据量等同于时间），是否有办法能够理由先验知识简化？*

*能否依据状态的信息进行决策的问题搞清楚了，状态转移的问题还没想清楚。*
